# Smart Incremental Scraping Architecture - AI Rules

## 🏗️ Core Architecture Principles

### **Respect Server Resources**
- **ALWAYS** implement rate limiting (max 10 requests/minute to BGU)
- **ALWAYS** add random delays (1-3 seconds) between requests
- **NEVER** implement per-user scraping - use centralized service
- **ALWAYS** use connection pooling for efficient resource usage
- **ALWAYS** implement exponential backoff for failed requests

### **3-Layer Update Strategy**
1. **Real-time Layer** (15-minute updates)
   - Critical assignments due in 7 days
   - New grades and urgent messages
   - Exam schedule changes
   - Priority: HIGH - Update frequently

2. **Dynamic Layer** (3-hour updates)
   - Course announcements
   - Calendar events
   - Assignment updates
   - Course material changes
   - Priority: MEDIUM - Update periodically

3. **Static Layer** (Daily updates)
   - Course lists and descriptions
   - Grade history
   - User profiles
   - Faculty/department information
   - Priority: LOW - Update infrequently

### **Centralized Scraping Service**
- **Single scraping service** for all 300 users
- **3-5 background workers** handling all requests
- **Shared cache** (Redis/in-memory) for common data
- **Job queue system** (BullMQ/Redis) for task management
- **No per-user scraping** - all users share the same service

## 🔧 Implementation Guidelines

### **Database Schema Requirements**
```sql
-- Job queue management
CREATE TABLE scraping_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_type VARCHAR(50) NOT NULL, -- 'real_time', 'dynamic', 'static'
    status VARCHAR(20) NOT NULL, -- 'pending', 'running', 'completed', 'failed'
    user_id UUID REFERENCES users(id),
    course_id UUID REFERENCES courses(id),
    created_at TIMESTAMP DEFAULT NOW(),
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT,
    retry_count INTEGER DEFAULT 0
);

-- Cache metadata for invalidation
CREATE TABLE cache_metadata (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    cache_key VARCHAR(255) NOT NULL,
    layer_type VARCHAR(20) NOT NULL, -- 'real_time', 'dynamic', 'static'
    last_updated TIMESTAMP DEFAULT NOW(),
    next_update TIMESTAMP NOT NULL,
    data_hash VARCHAR(64), -- For detecting changes
    UNIQUE(cache_key)
);
```

### **API Endpoints Structure**
```typescript
// Scraping management endpoints
POST /api/scraping/sync - Trigger manual sync
GET /api/scraping/status - Get sync status
GET /api/scraping/jobs - Get job queue status
POST /api/scraping/jobs - Create new scraping job
GET /api/scraping/jobs/[id] - Get job details
PUT /api/scraping/jobs/[id] - Update job status

// Data endpoints with caching
GET /api/courses - Fetch courses (cached)
GET /api/assignments - Fetch assignments (real-time layer)
GET /api/grades - Fetch grades (real-time layer)
GET /api/announcements - Fetch announcements (dynamic layer)
```

### **Background Worker Implementation**
```python
# workers/scraping_worker.py
import asyncio
import random
import time
from typing import Dict, Any
from datetime import datetime, timedelta

class SmartScrapingWorker:
    def __init__(self):
        self.rate_limit = 10  # requests per minute
        self.request_count = 0
        self.last_reset = datetime.now()
        self.cache = {}  # Shared cache
    
    async def process_job(self, job_data: Dict[str, Any]):
        """Process scraping job with rate limiting"""
        # Check rate limit
        if not self.check_rate_limit():
            await asyncio.sleep(60)  # Wait for rate limit reset
        
        # Add random delay
        delay = random.uniform(1, 3)
        await asyncio.sleep(delay)
        
        # Process based on layer type
        layer_type = job_data.get('layer_type', 'dynamic')
        
        if layer_type == 'real_time':
            await self.process_real_time_layer(job_data)
        elif layer_type == 'dynamic':
            await self.process_dynamic_layer(job_data)
        elif layer_type == 'static':
            await self.process_static_layer(job_data)
    
    def check_rate_limit(self) -> bool:
        """Check if we're within rate limits"""
        now = datetime.now()
        if (now - self.last_reset).seconds >= 60:
            self.request_count = 0
            self.last_reset = now
        
        if self.request_count >= self.rate_limit:
            return False
        
        self.request_count += 1
        return True
    
    async def process_real_time_layer(self, job_data: Dict[str, Any]):
        """Process real-time layer (15-minute updates)"""
        # Fetch critical data
        assignments = await self.fetch_assignments_due_soon()
        grades = await self.fetch_new_grades()
        urgent_messages = await self.fetch_urgent_messages()
        
        # Update cache with short TTL
        self.cache['real_time_data'] = {
            'assignments': assignments,
            'grades': grades,
            'urgent_messages': urgent_messages,
            'updated_at': datetime.now(),
            'ttl': 900  # 15 minutes
        }
    
    async def process_dynamic_layer(self, job_data: Dict[str, Any]):
        """Process dynamic layer (3-hour updates)"""
        # Fetch course updates
        announcements = await self.fetch_course_announcements()
        calendar_events = await self.fetch_calendar_events()
        assignment_updates = await self.fetch_assignment_updates()
        
        # Update cache with medium TTL
        self.cache['dynamic_data'] = {
            'announcements': announcements,
            'calendar_events': calendar_events,
            'assignment_updates': assignment_updates,
            'updated_at': datetime.now(),
            'ttl': 10800  # 3 hours
        }
    
    async def process_static_layer(self, job_data: Dict[str, Any]):
        """Process static layer (daily updates)"""
        # Fetch stable data
        course_lists = await self.fetch_course_lists()
        grade_history = await self.fetch_grade_history()
        user_profiles = await self.fetch_user_profiles()
        
        # Update cache with long TTL
        self.cache['static_data'] = {
            'course_lists': course_lists,
            'grade_history': grade_history,
            'user_profiles': user_profiles,
            'updated_at': datetime.now(),
            'ttl': 86400  # 24 hours
        }
```

### **Caching Strategy**
```typescript
// lib/cache-manager.ts
interface CacheLayer {
  real_time: CacheData;
  dynamic: CacheData;
  static: CacheData;
}

interface CacheData {
  data: any;
  updated_at: Date;
  ttl: number; // seconds
  next_update: Date;
}

class SmartCacheManager {
  private cache: CacheLayer = {
    real_time: { data: null, updated_at: null, ttl: 900, next_update: null },
    dynamic: { data: null, updated_at: null, ttl: 10800, next_update: null },
    static: { data: null, updated_at: null, ttl: 86400, next_update: null }
  };

  async getData(layer: keyof CacheLayer, forceRefresh = false) {
    const cacheData = this.cache[layer];
    
    // Check if cache is valid
    if (!forceRefresh && this.isCacheValid(cacheData)) {
      return cacheData.data;
    }
    
    // Trigger background update
    this.scheduleUpdate(layer);
    
    // Return stale data if available, otherwise null
    return cacheData.data || null;
  }

  private isCacheValid(cacheData: CacheData): boolean {
    if (!cacheData.updated_at) return false;
    
    const now = new Date();
    const age = (now.getTime() - cacheData.updated_at.getTime()) / 1000;
    return age < cacheData.ttl;
  }

  private scheduleUpdate(layer: keyof CacheLayer) {
    // Add job to queue for background update
    // This ensures we don't block the user while updating
  }
}
```

## 🚨 Critical Rules

### **NEVER Do This:**
- ❌ Implement per-user scraping
- ❌ Make requests without rate limiting
- ❌ Skip random delays between requests
- ❌ Update all data on every request
- ❌ Block user interface during scraping
- ❌ Ignore server errors and retry limits

### **ALWAYS Do This:**
- ✅ Use centralized scraping service
- ✅ Implement rate limiting (10 req/min)
- ✅ Add random delays (1-3 seconds)
- ✅ Use 3-layer update strategy
- ✅ Cache data appropriately
- ✅ Handle errors gracefully
- ✅ Monitor performance metrics

### **Performance Requirements:**
- **Response time**: < 2 seconds for cached data
- **Background updates**: Non-blocking
- **Error recovery**: Automatic retry with exponential backoff
- **Resource usage**: Minimal memory and CPU impact
- **Scalability**: Support 300 concurrent users

## 🔄 Development Workflow

### **Phase 1: MVP with Bulk Sync (Week 1-2)**
1. Implement basic scraping service
2. Add simple caching layer
3. Basic rate limiting
4. Manual sync functionality

### **Phase 2: Incremental System (Week 3-4)**
1. Implement 3-layer update strategy
2. Add job queue system
3. Implement background workers
4. Add smart cache invalidation

### **Phase 3: Scale to 300 Users (Week 5)**
1. Optimize caching strategy
2. Add monitoring and alerting
3. Performance testing
4. Load balancing

## 📊 Monitoring & Metrics

### **Key Metrics to Track:**
- Request success rate
- Average response time
- Cache hit rate
- Queue length and processing time
- Error rates by layer
- Rate limit violations

### **Alerting Rules:**
- Error rate > 5%
- Response time > 5 seconds
- Cache hit rate < 80%
- Queue length > 100 jobs
- Rate limit violations > 10/hour

---

**Remember**: The goal is to provide a smooth user experience while respecting BGU's server resources. Always prioritize efficiency and scalability over immediate data freshness.
description:
globs:
alwaysApply: false
---
